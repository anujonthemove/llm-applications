{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Question Answer pairs from a PDF using LangGraph - iterative refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "import operator\n",
    "from typing import List, Literal, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain import hub\n",
    "\n",
    "from decouple import config\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = config('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load LLM and Embeddings\n",
    "\n",
    "* Here I am using LM Studio for locally running the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=\"sk-1234\", \n",
    "    base_url=\"http://localhost:1234/v1/\", \n",
    "    model=\"nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q4_K_M.gguf\",\n",
    "    check_embedding_ctx_length=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/home/acer/Downloads/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf\"\n",
    "# pdf_path = \"/home/acer/workspace/personal/llms/data/sdgp.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-2: Create Question and Answer splitter\n",
    "\n",
    "1. Create question splitter\n",
    "2. From question splitter, create answer splitter. This is to ensure that you generate answers from the question context only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "questions = question_splitter.split_documents(docs)\n",
    "\n",
    "print(questions[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "answers = answer_splitter.split_documents(questions)\n",
    "\n",
    "print(answers[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3: Create question generation prompt, prompt template, initial question generation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_question_generation_prompt = \"\"\"\n",
    "You are an expert in creating questions based on a provided text corpus.\n",
    "Your goal is to help prepare questions for student tests and exams.\n",
    "You should do this by asking questions from the text provided below:\n",
    "\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Please ensure that you do not loose any important information.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_question_prompt_template = PromptTemplate(template=initial_question_generation_prompt, input_variables=[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_question_generation_chain = initial_question_prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-4: Create question refinemnent prompt, prompt template, question refinement chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_question_prompt = \"\"\"\n",
    "You are an expert in creating and refining questions based on a provided text corpus.\n",
    "Your goal is to help a faculty prepare questions for student tests and exams.\n",
    "Here are some of the questions already prepared: \\n {existing_questions} \\n.\n",
    "\n",
    "You have an option to refine the existing questions or add new ones if necessary based on the new context provided below: \\n\n",
    "When you have a refined question, just replace the existing question with the new one. \n",
    "At the end provide a comlpete list of only the refined questions with a question mark.\n",
    "\\n ------------ \\n\n",
    "\n",
    "{context}\n",
    "\n",
    "\\n ------------ \\n\n",
    "\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_question_prompt_template = PromptTemplate(template=refine_question_prompt, input_variables=[\"context\", \"existing_questions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_summary_chain = refine_question_prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-5: Create State Graph using LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create state dictionary - shared datastructure between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    contents: List[str]\n",
    "    index: int\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the functions to be executed for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_initial_summary(state: State, config: RunnableConfig):\n",
    "    summary = await initial_question_generation_chain.ainvoke(\n",
    "        state[\"contents\"][0],\n",
    "        config,\n",
    "    )\n",
    "\n",
    "    return {\"summary\": summary, \"index\": 1}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def refine_summary(state: State, config: RunnableConfig):\n",
    "    content = state[\"contents\"][state[\"index\"]]\n",
    "    summary = await refine_summary_chain.ainvoke(\n",
    "        {\"existing_questions\" : state[\"summary\"], \"context\" : content},\n",
    "        config,\n",
    "    )\n",
    "\n",
    "    return {\"summary\": summary, \"index\": state[\"index\"] + 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_refine(state: State) -> Literal[\"refine_summary\", END]:\n",
    "    if state[\"index\"] >= len(state[\"contents\"]):\n",
    "        return END\n",
    "    else:\n",
    "        return \"refine_summary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-6: Create a Graph, add node, add edges, compile, and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "graph.add_node(\"generate_initial_summary\", generate_initial_summary)\n",
    "graph.add_node(\"refine_summary\", refine_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_edge(START, \"generate_initial_summary\")\n",
    "graph.add_conditional_edges(\"generate_initial_summary\", should_refine)\n",
    "graph.add_conditional_edges(\"refine_summary\", should_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-7: Execute the graph to generate initial questions and then refine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = []\n",
    "async for step in app.astream(\n",
    "    {\"contents\": [doc.page_content for doc in questions]},\n",
    "    stream_mode = \"values\",\n",
    "):\n",
    "\n",
    "    if summary := step.get(\"summary\"):\n",
    "        print(summary)\n",
    "        questions_list.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = questions_list[-1].split(\"\\n\")\n",
    "filtered_ques_list = [element for element in ques_list if element.endswith('?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_ques_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-8: Store answers in vector store, create a qa chain, ask generated questions, generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(documents=answers, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in filtered_ques_list:\n",
    "    print(f\"Question: {question}\")\n",
    "    response = qa_chain.invoke(question)\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Work\n",
    "* Refine question generation prompt to generate better questions\n",
    "* Structured output?\n",
    "* Recursion limit set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
